---
title: "regression"
output: html_document
date: "2025-11-06"
---
### Packages needed
```{r}
#install.packages("remotes")
#remotes::install_github("gabrielrvsc/HDeconometrics")
library(HDeconometrics)
library(sandwich) 
library(hdm)
library(tidyverse)
library(leaps)

```
### Load the files
```{r}
setwd("/Users/yizhouhang/Documents/Y4S1/EC4308/ec4308 project/src")
df_1m <- read_csv("../data/df_1m_lag.csv", show_col_types = FALSE)
df_3m <- read_csv("../data/df_3m_lag.csv", show_col_types = FALSE)
df_6m <- read_csv("../data/df_6m_lag.csv", show_col_types = FALSE)
df_12m <- read_csv("../data/df_12m_lag.csv", show_col_types = FALSE)
```

```{r}
clean <- df_1m
clean
```

### Train Test Split
```{r}
# test is >= 2012-01-01 
split_date <- as.Date("2012-01-01")
is_test  <- clean$date >= split_date
is_train <- !is_test

y <- clean$Y_log_return_1m
X <- clean %>% select(-date, -current_fx_level,-fx_level_fwd_1m,-Y_log_return_1m) %>% as.matrix()

X_tr <- X[is_train, , drop = FALSE]
y_tr <- y[is_train]
X_te <- X[is_test,  , drop = FALSE]
y_te <- y[is_test]
```

### Standardization
```{r}
train_center <- colMeans(X_tr)
train_scale  <- apply(X_tr, 2, sd)

# drop zero-variance columns 
zv <- which(is.na(train_scale) | train_scale == 0)
if (length(zv) > 0) {
  X_tr <- X_tr[, -zv, drop = FALSE]
  X_te <- X_te[, -zv, drop = FALSE]
  train_center <- train_center[-zv]
  train_scale  <- train_scale[-zv]
}

scale_with <- function(M, center, scale) {
  sweep(sweep(M, 2, center, "-"), 2, scale, "/")
}

X_tr_s <- scale_with(X_tr, train_center, train_scale)
X_te_s <- scale_with(X_te, train_center, train_scale)

```

### OLS Summary
```{r}
## Fit OLS on standardized features
df_tr <- data.frame(y = y_tr, as.data.frame(X_tr_s))
df_te <- data.frame(y = y_te, as.data.frame(X_te_s))
ols   <- lm(y ~ ., data = df_tr)

## Predictions
pred_tr <- predict(ols, newdata = df_tr)
pred_te <- predict(ols, newdata = df_te)

## Metrics
rmse <- function(a, p) sqrt(mean((a - p)^2))

rmse_tr <- rmse(y_tr, pred_tr)
rmse_te <- rmse(y_te, pred_te)

# in-sample R^2 from lm summary
r2_in   <- summary(ols)$r.squared

# out-of-sample R^2 
r2_oos  <- 1 - sum((y_te - pred_te)^2) / sum((y_te - mean(y_tr))^2)

metrics <- list(
  rmse_train = rmse_tr,
  rmse_test  = rmse_te,
  r2_in      = r2_in,
  r2_oos     = r2_oos
)

metrics
```
### subset selection: Forward Stepwise + AIC/BIC
```{r}
forward_ic_ts <- function(X_tr_s, y_tr, X_te_s, y_te, nvmax = 25, max_iter = 2) {
  stopifnot(identical(colnames(X_tr_s), colnames(X_te_s)))
  n  <- NROW(X_tr_s)
  p  <- NCOL(X_tr_s)
  nv <- min(nvmax, p)

  df_tr <- data.frame(y = y_tr, as.data.frame(X_tr_s))
  df_te <- data.frame(y = y_te, as.data.frame(X_te_s))
  regfit <- regsubsets(y ~ ., data = df_tr, nvmax = nv, method = "forward")
  s      <- summary(regfit) 
  rss    <- s$rss          

  ic_vec <- function(rss_vec, n, sigma2, pen) rss_vec / n + pen * sigma2 * ((1:nv) / n)


  sig2 <- stats::var(y_tr)

  pick_k <- function(sig2) {
    BIC <- ic_vec(rss, n, sig2, log(n))
    AIC <- ic_vec(rss, n, sig2, 2)
    list(k_bic = which.min(BIC), k_aic = which.min(AIC), BIC = BIC, AIC = AIC)
  }

  sel_hist <- list()
  for (iter in 0:max_iter) {
    sel <- pick_k(sig2)
    sel_hist[[iter + 1]] <- list(iter = iter, sigma2 = sig2,
                                 k_bic = sel$k_bic, k_aic = sel$k_aic)
  
    sig2_bic <- rss[sel$k_bic] / (n - sel$k_bic - 1)
    sig2_aic <- rss[sel$k_aic] / (n - sel$k_aic - 1)


    if (iter > 0) {
      if (sel$k_bic == prev_k_bic && sel$k_aic == prev_k_aic) break
    }
    if (iter == max_iter) break

    prev_k_bic <- sel$k_bic
    prev_k_aic <- sel$k_aic
    
    sig2 <- sig2_bic
  }

  k_bic <- sel$k_bic
  k_aic <- sel$k_aic


  MM_tr <- model.matrix(y ~ ., data = df_tr)
  MM_te <- model.matrix(y ~ ., data = df_te)


  predict_subset <- function(obj, newMM, id) {
    cf <- coef(obj, id = id)
    as.numeric(newMM[, names(cf), drop = FALSE] %*% cf)
  }

  # metrics
  rmse <- function(y, p) {
    idx <- is.finite(y) & is.finite(p)
    sqrt(mean((y[idx] - p[idx])^2))
  }
  r2_in <- function(y, p) {
    idx <- is.finite(y) & is.finite(p)
    y <- y[idx]; p <- p[idx]
    denom <- sum((y - mean(y))^2)
    if (denom == 0) return(NA_real_)
    1 - sum((y - p)^2) / denom
  }
  r2_oos <- function(y_te, p_te, y_tr) {
    idx <- is.finite(y_te) & is.finite(p_te)
    y_te <- y_te[idx]; p_te <- p_te[idx]
    denom <- sum((y_te - mean(y_tr))^2)   # baseline = TRAIN mean
    if (denom == 0) return(NA_real_)
    1 - sum((y_te - p_te)^2) / denom
  }


  pred_tr_bic <- predict_subset(regfit, MM_tr, k_bic)
  pred_te_bic <- predict_subset(regfit, MM_te, k_bic)

  metrics_bic <- list(
    k          = k_bic,
    rmse_train = rmse(y_tr, pred_tr_bic),
    rmse_test  = rmse(y_te, pred_te_bic),
    r2_in      = r2_in(y_tr, pred_tr_bic),
    r2_oos     = r2_oos(y_te, pred_te_bic, y_tr)
    vars    = names(coef(regfit, id = k_bic))  
  )

  pred_tr_aic <- predict_subset(regfit, MM_tr, k_aic)
  pred_te_aic <- predict_subset(regfit, MM_te, k_aic)

  metrics_aic <- list(
    k          = k_aic,
    rmse_train = rmse(y_tr, pred_tr_aic),
    rmse_test  = rmse(y_te, pred_te_aic),
    r2_in      = r2_in(y_tr, pred_tr_aic),
    r2_oos     = r2_oos(y_te, pred_te_aic, y_tr)
    vars    = names(coef(regfit, id = k_aic))
  )


  cat(sprintf(
    "\n[Forward stepwise, nvmax=%d]\nBIC pick (k=%d)\n  Train RMSE: %.5f | R^2: %.3f\n  Test  RMSE: %.5f | R^2_oos: %.3f\n",
    nv, metrics_bic$k,
    metrics_bic$rmse_train, metrics_bic$r2_in,
    metrics_bic$rmse_test,  metrics_bic$r2_oos
  ))
  cat(sprintf(
    "\nAIC/Cp pick (k=%d)\n  Train RMSE: %.5f | R^2: %.3f\n  Test  RMSE: %.5f | R^2_oos: %.3f\n",
    metrics_aic$k,
    metrics_aic$rmse_train, metrics_aic$r2_in,
    metrics_aic$rmse_test,  metrics_aic$r2_oos
  ))

 invisible(list(
  method     = "forward",
  nvmax      = nv,
  iter_trace = sel_hist,
  # return predictions & var names for BIC and AIC picks
  bic_model  = c(
    list(
      k       = k_bic,
      pred_tr = pred_tr_bic,
      pred_te = pred_te_bic,
      vars    = setdiff(names(coef(regfit, id = k_bic)), "(Intercept)")
    ),
    metrics_bic
  ),
  aic_model  = c(
    list(
      k       = k_aic,
      pred_tr = pred_tr_aic,
      pred_te = pred_te_aic,
      vars    = setdiff(names(coef(regfit, id = k_aic)), "(Intercept)")
    ),
    metrics_aic
  )
))

}
```

### Fit the model
```{r}
# Fit
res <- forward_ic_ts(X_tr_s, y_tr, X_te_s, y_te, nvmax = 25, max_iter = 2)

pred_te_bic <- res$bic_model$pred_te
pred_tr_bic <- res$bic_model$pred_tr
bic_k       <- res$bic_model$k
bic_vars    <- res$bic_model$vars  
```

### DM test with random walk
```{r}
# Random-walk forecast for returns = 0
rw_pred     <- rep(0, length(y_te))

# RW OOS R^2 (baseline uses TRAIN mean)
rw_r2_oos   <- 1 - sum((y_te - rw_pred)^2) / sum((y_te - mean(y_tr))^2)

# Diebold–Mariano test (model vs RW, squared-error, 1-step-ahead)
# install.packages("forecast")
library(forecast)
dm_rw_vs_bic <- dm.test(e1 = y_te - pred_te_bic, e2 = y_te - rw_pred, h = 1, power = 2)

# Print
cat("\n--- BIC pick details ---\n")
cat(sprintf("k = %d\n", bic_k))
cat("Variables:\n"); print(bic_vars)
cat(sprintf("\nRW R2_oos = %.3f\n", rw_r2_oos))
cat("\nDM test (model vs RW):\n"); print(dm_rw_vs_bic)

```
```{r}
library(HDeconometrics)
library(forecast)
library(dplyr)

split_date <- as.Date("2012-01-01")

# --- build y, X exactly like your snippet ---
y <- clean$Y_log_return_1m
X <- clean %>% select(-date, -current_fx_level, -fx_level_fwd_1m, -Y_log_return_1m) %>% as.matrix()

is_test  <- clean$date >= split_date
is_train <- !is_test

y_tr <- y[is_train]; y_te <- y[is_test]
X_tr <- X[is_train, , drop = FALSE]
X_te <- X[is_test,  , drop = FALSE]

# drop zero-variance cols using TRAIN only (no leakage)
sds   <- apply(X_tr, 2, sd)
keep  <- which(is.finite(sds) & sds > 0)
X_tr  <- X_tr[, keep, drop = FALSE]
X_te  <- X_te[, keep, drop = FALSE]
cn    <- make.names(colnames(X_tr), unique = TRUE)
colnames(X_tr) <- colnames(X_te) <- cn

# standardize with TRAIN stats only
mu  <- colMeans(X_tr)
sdv <- apply(X_tr, 2, sd)
scale_with <- function(M, center, scale) sweep(sweep(M, 2, center, "-"), 2, scale, "/")
X_tr_s <- scale_with(X_tr, mu, sdv)
X_te_s <- scale_with(X_te, mu, sdv)

# fit IC-LASSO (alpha=1)
fit <- ic.glmnet(X_tr_s, y_tr, family = "gaussian", alpha = 1, standardize = FALSE)
b   <- as.numeric(coef(fit))            # [beta0, beta_1..p]
beta0 <- b[1]; beta <- b[-1]

# predictions
pred_te <- as.numeric(drop(X_te_s %*% beta + beta0))

# metrics
rmse <- function(p, t) sqrt(mean((t - p)^2))
r2_test <- 1 - sum((y_te - pred_te)^2) / sum((y_te - mean(y_te))^2)       # test-mean baseline
r2_oos  <- 1 - sum((y_te - pred_te)^2) / sum((y_te - mean(y_tr))^2)       # train-mean baseline (recommended)

cat(sprintf("\n[Train/Test IC-LASSO]\nRMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            rmse(pred_te, y_te), r2_test, r2_oos))

# DM vs Random Walk (returns -> 0)
rw_pred <- rep(0, length(y_te))
dm_tt   <- forecast::dm.test(e1 = y_te - pred_te, e2 = y_te - rw_pred, h = 1, power = 2)
print(dm_tt)

# selected variable names (from TRAIN)
sel_idx   <- which(beta != 0)
sel_names <- if (length(sel_idx)) colnames(X_tr_s)[sel_idx] else character(0)
cat("Selected vars:", if (length(sel_names)) paste(sel_names, collapse=", ") else "<none>", "\n")

```



```{r}
library(HDeconometrics)
library(forecast)

# full y, X exactly per your rule
y_full <- clean$Y_log_return_1m
X_full <- clean %>% select(-date, -current_fx_level, -fx_level_fwd_1m, -Y_log_return_1m) %>% as.matrix()
dates  <- as.Date(clean$date)

# test starts at split_date
test_start <- which(dates >= split_date)[1]
stopifnot(!is.na(test_start))

# rolling window length (months)
W <- 120
preds <- rep(NA_real_, length(y_full))

scale_with <- function(M, center, scale) sweep(sweep(M, 2, center, "-"), 2, scale, "/")

for (t in seq(test_start, length(y_full))) {
  # need at least W obs before t for rolling; skip until then
  if (t - 1 < W) next
  idx_tr <- (t - W):(t - 1)

  X_tr <- X_full[idx_tr, , drop = FALSE]
  y_tr <- y_full[idx_tr]
  x_te <- X_full[t, , drop = FALSE]

  # drop zero-variance in TRAIN window only
  sds  <- apply(X_tr, 2, sd)
  keep <- which(is.finite(sds) & sds > 0)
  if (!length(keep)) { preds[t] <- mean(y_tr); next }
  X_tr <- X_tr[, keep, drop = FALSE]
  x_te <- x_te[, keep, drop = FALSE]

  # standardize within TRAIN window
  mu  <- colMeans(X_tr)
  sdv <- apply(X_tr, 2, sd)
  X_tr_s <- scale_with(X_tr, mu, sdv)
  x_te_s <- scale_with(x_te, mu, sdv)

  # fit IC-LASSO and predict
  fit <- ic.glmnet(X_tr_s, y_tr, family = "gaussian", alpha = 1, standardize = FALSE)
  b   <- as.numeric(coef(fit))
  preds[t] <- as.numeric(drop(x_te_s %*% b[-1] + b[1]))
}

# test-period evaluation (only where we have forecasts)
idx_eval <- which(dates >= split_date & is.finite(preds))
y_eval   <- y_full[idx_eval]
y_hat    <- preds[idx_eval]

rmse_roll  <- sqrt(mean((y_eval - y_hat)^2))
# two ways to report R^2; be explicit about baseline:
r2_test    <- 1 - sum((y_eval - y_hat)^2) / sum((y_eval - mean(y_eval))^2)         # test-mean
train_mean <- mean(y_full[dates < split_date], na.rm = TRUE)
r2_oos     <- 1 - sum((y_eval - y_hat)^2) / sum((y_eval - train_mean)^2)           # train-mean (conservative)

cat(sprintf("\n[Rolling IC-LASSO] W=%d\nRMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            W, rmse_roll, r2_test, r2_oos))

# DM vs RW (0 returns)
rw_eval <- rep(0, length(y_eval))
dm_roll <- forecast::dm.test(e1 = y_eval - y_hat, e2 = y_eval - rw_eval, h = 1, power = 2)
print(dm_roll)

```


###post LASSO
```{r}
post_lasso_ols <- function(X_tr_s, y_tr, X_te_s, y_te, alpha = 1) {
  fit_lasso <- HDeconometrics::ic.glmnet(
    X_tr_s, y_tr, family = "gaussian", alpha = alpha, standardize = FALSE
  )
  b <- as.numeric(coef(fit_lasso))
  beta0 <- b[1]; beta <- b[-1]

  sel_idx   <- which(beta != 0)
  sel_names <- colnames(X_tr_s)[sel_idx]

  if (length(sel_names) == 0) {
    message("No variables selected → Post-LASSO not possible.")
    return(NULL)
  }

  # ensure the same names exist in test
  missing_in_te <- setdiff(sel_names, colnames(X_te_s))
  if (length(missing_in_te)) {
    stop("Selected vars not found in TEST: ", paste(missing_in_te, collapse = ", "))
  }

  df_tr_sub <- data.frame(y = y_tr, X_tr_s[, sel_names, drop = FALSE])
  df_te_sub <- data.frame(X_te_s[, sel_names, drop = FALSE])

  fit_post  <- lm(y ~ ., data = df_tr_sub)

  # predict via manual matrix product (avoids formula env pitfalls)
  X_te_mm <- cbind(`(Intercept)` = 1, as.matrix(df_te_sub))
  coefs   <- coef(fit_post)
  # align names; if any coefficient is missing in X_te_mm (shouldn't), fill with 0
  coefs[setdiff(names(coefs), colnames(X_te_mm))] <- 0
  X_te_mm <- X_te_mm[, names(coefs), drop = FALSE]
  pred_te <- as.numeric(X_te_mm %*% coefs)

  # metrics
  tr_hat  <- fitted(fit_post)
  cat("\n---- Post-LASSO OLS ----\n")
  cat("Selected (", length(sel_names), "): ",
      paste(sel_names, collapse = ", "), "\n", sep = "")

  cat(sprintf("Train RMSE: %.5f   R^2_train: %.3f\n",
              rmse(tr_hat, y_tr),
              r2_test(tr_hat, y_tr)))
  cat(sprintf("Test  RMSE: %.5f   R^2_test : %.3f   R^2_oos: %.3f\n",
              rmse(pred_te, y_te),
              r2_test(pred_te, y_te),
              r2_oos(pred_te, y_te, y_tr)))

  invisible(list(model = fit_post,
                 selected = sel_names,
                 pred_te = pred_te))
}

res_post <- post_lasso_ols(X_tr_s, y_tr, X_te_s, y_te, alpha = 1)
```

### Ridge and Elastic Net
```{r}
library(HDeconometrics)
library(forecast)
library(dplyr)

split_date <- as.Date("2012-01-01")

# exact target/features you asked for
y_full <- clean$Y_log_return_1m
X_full <- clean %>%
  select(-date, -current_fx_level, -fx_level_fwd_1m, -Y_log_return_1m) %>%
  as.matrix()
dates  <- as.Date(clean$date)

rmse <- function(p, t) sqrt(mean((t - p)^2))


train_test_ic_alpha <- function(X_full, y_full, dates, split_date, alpha = 1) {
  is_test  <- dates >= split_date
  is_train <- !is_test

  y_tr <- y_full[is_train]; y_te <- y_full[is_test]
  X_tr <- X_full[is_train, , drop = FALSE]
  X_te <- X_full[is_test,  , drop = FALSE]

  # drop zero-variance cols on TRAIN only
  sds  <- apply(X_tr, 2, sd)
  keep <- which(is.finite(sds) & sds > 0)
  X_tr <- X_tr[, keep, drop = FALSE]
  X_te <- X_te[, keep, drop = FALSE]

  # standardize with TRAIN stats only
  mu  <- colMeans(X_tr)
  sdv <- apply(X_tr, 2, sd)
  scale_with <- function(M, center, scale) sweep(sweep(M, 2, center, "-"), 2, scale, "/")
  X_tr_s <- scale_with(X_tr, mu, sdv)
  X_te_s <- scale_with(X_te, mu, sdv)

  # fit IC-GLMNET
  fit <- ic.glmnet(X_tr_s, y_tr, family = "gaussian",
                   alpha = alpha, standardize = FALSE)
  b <- as.numeric(coef(fit))  # [beta0, beta_1..p]
  pred_te <- as.numeric(drop(X_te_s %*% b[-1] + b[1]))

  # metrics
  r2_test <- 1 - sum((y_te - pred_te)^2) / sum((y_te - mean(y_te))^2)
  r2_oos  <- 1 - sum((y_te - pred_te)^2) / sum((y_te - mean(y_tr))^2)

  # DM vs RW=0
  rw_pred <- rep(0, length(y_te))
  dm_res  <- forecast::dm.test(e1 = y_te - pred_te, e2 = y_te - rw_pred, h = 1, power = 2)

  list(
    rmse_test = rmse(pred_te, y_te),
    r2_test   = r2_test,
    r2_oos    = r2_oos,
    dm_test   = dm_res
  )
}

ridge_tt <- train_test_ic_alpha(X_full, y_full, dates, split_date, alpha = 0)
en_tt    <- train_test_ic_alpha(X_full, y_full, dates, split_date, alpha = 0.5)

cat(sprintf("\n[Train/Test RIDGE]  RMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            ridge_tt$rmse_test, ridge_tt$r2_test, ridge_tt$r2_oos))
print(ridge_tt$dm_test)

cat(sprintf("\n[Train/Test EN(0.5)] RMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            en_tt$rmse_test, en_tt$r2_test, en_tt$r2_oos))
print(en_tt$dm_test)

```

```{r}
rolling_ic_alpha <- function(X_full, y_full, dates, split_date,
                             alpha = 1, mode = c("rolling","expanding"),
                             window = 120, min_train = 60) {
  mode <- match.arg(mode)
  n <- NROW(X_full)
  preds <- rep(NA_real_, n)

  test_start <- which(dates >= split_date)[1]
  if (is.na(test_start)) stop("split_date is after all observations.")
  start_t <- max(test_start, min_train + 1)

  scale_with <- function(M, center, scale) sweep(sweep(M, 2, center, "-"), 2, scale, "/")

  for (t in seq(from = start_t, to = n)) {
    if (mode == "rolling") {
      if (t - 1 < window) next
      idx_tr <- (t - window):(t - 1)
    } else {
      if (t - 1 < min_train) next
      idx_tr <- 1:(t - 1)
    }
    X_tr <- X_full[idx_tr, , drop = FALSE]
    y_tr <- y_full[idx_tr]
    x_te <- X_full[t, , drop = FALSE]

    # drop zero-variance cols in TRAIN window
    sds  <- apply(X_tr, 2, sd)
    keep <- which(is.finite(sds) & sds > 0)
    if (!length(keep)) { preds[t] <- mean(y_tr); next }
    X_tr <- X_tr[, keep, drop = FALSE]
    x_te <- x_te[, keep, drop = FALSE]

    # standardize within TRAIN window
    mu  <- colMeans(X_tr)
    sdv <- apply(X_tr, 2, sd)
    X_tr_s <- scale_with(X_tr, mu, sdv)
    x_te_s <- scale_with(x_te, mu, sdv)

    fit <- ic.glmnet(X_tr_s, y_tr, family = "gaussian",
                     alpha = alpha, standardize = FALSE)
    b <- as.numeric(coef(fit))
    preds[t] <- as.numeric(drop(x_te_s %*% b[-1] + b[1]))
  }

  # evaluate on test span where preds exist
  idx_eval <- which(dates >= split_date & is.finite(preds))
  y_eval   <- y_full[idx_eval]
  y_hat    <- preds[idx_eval]

  rmse_te  <- rmse(y_hat, y_eval)
  r2_test  <- 1 - sum((y_eval - y_hat)^2) / sum((y_eval - mean(y_eval))^2)
  train_mean <- mean(y_full[dates < split_date], na.rm = TRUE)
  r2_oos   <- 1 - sum((y_eval - y_hat)^2) / sum((y_eval - train_mean)^2)

  # DM vs RW(=0)
  rw_eval  <- rep(0, length(y_eval))
  dm_res   <- forecast::dm.test(e1 = y_eval - y_hat, e2 = y_eval - rw_eval, h = 1, power = 2)

  list(rmse_test = rmse_te, r2_test = r2_test, r2_oos = r2_oos, dm_test = dm_res,
       idx_eval = idx_eval, y_eval = y_eval, y_hat = y_hat)
}

# Rolling, W = 120 months
ridge_roll <- rolling_ic_alpha(X_full, y_full, dates, split_date,
                               alpha = 0, mode = "rolling", window = 120)
en_roll    <- rolling_ic_alpha(X_full, y_full, dates, split_date,
                               alpha = 0.5, mode = "rolling", window = 120)

cat(sprintf("\n[Rolling RIDGE]   RMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            ridge_roll$rmse_test, ridge_roll$r2_test, ridge_roll$r2_oos))
print(ridge_roll$dm_test)

cat(sprintf("\n[Rolling EN(0.5)] RMSE=%.5f  R^2_test=%.3f  R^2_oos=%.3f\n",
            en_roll$rmse_test, en_roll$r2_test, en_roll$r2_oos))
print(en_roll$dm_test)

# (Optional) Expanding instead of rolling:
# ridge_exp <- rolling_ic_alpha(X_full, y_full, dates, split_date, alpha = 0,   mode = "expanding")
# en_exp    <- rolling_ic_alpha(X_full, y_full, dates, split_date, alpha = 0.5, mode = "expanding")

```

