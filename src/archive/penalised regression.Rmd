---
title: "penalised regression"
output: html_document
date: "2025-11-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tibble)
library(purrr)
library(readr)
library(leaps)
library(glmnet)       
library(forecast)      
```

```{r}
setwd("/Users/yizhouhang/Documents/Y4S1/EC4308/ec4308 project/src")
df_1m  <- read_csv("../data/df_1m_lag.csv",  show_col_types = FALSE)
df_3m  <- read_csv("../data/df_3m_lag.csv",  show_col_types = FALSE)
df_6m  <- read_csv("../data/df_6m_lag.csv",  show_col_types = FALSE)
df_12m <- read_csv("../data/df_12m_lag.csv", show_col_types = FALSE)
```

### Preparing data

```{r}
rmse <- function(p, t) sqrt(mean((t - p)^2))
r2_insample <- function(y, p) { den <- sum((y - mean(y))^2); if (den == 0) return(NA_real_); 1 - sum((y - p)^2) / den }
r2_oos_trainmean <- function(y_te, p_te, y_tr) { den <- sum((y_te - mean(y_tr))^2); if (den == 0) return(NA_real_); 1 - sum((y_te - p_te)^2) / den }
scale_with <- function(M, center, scale) sweep(sweep(M, 2, center, "-"), 2, scale, "/")

# directional accuracy
dir_acc <- function(y, p, count_zeros = FALSE) {
  sy <- sign(y); sp <- sign(p)

  if (count_zeros) {
    keep <- is.finite(sy) & is.finite(sp)
  } else {
    keep <- is.finite(sy) & is.finite(sp) & (sy != 0) & (sp != 0)
  }

  n <- sum(keep, na.rm = TRUE)
  if (n == 0) return(list(da = NA_real_, n = 0, pval = NA_real_))

  hits <- sum(sy[keep] == sp[keep], na.rm = TRUE)
  # guard types for binom.test
  bt <- binom.test(as.integer(hits), as.integer(n), p = 0.5, alternative = "two.sided")

  list(da = hits / n, n = n, pval = as.numeric(bt$p.value))
}

# drop linearly dependent columns using QR on TRAIN only; propagate to TEST
qr_prune_cols <- function(X_tr, X_te) {
  qx <- qr(X_tr)
  keep_idx <- sort(qx$pivot[seq_len(qx$rank)])
  list(X_tr = X_tr[, keep_idx, drop = FALSE],
       X_te = X_te[, keep_idx, drop = FALSE],
       keep_names = colnames(X_tr)[keep_idx])
}

# ensure model.matrix columns match regsubsets coefs and pad zeros for missing
safe_coef_predict <- function(regfit, X_s, id) {
  cf <- coef(regfit, id = id)                        # named vector (Intercept + vars)
  mm <- cbind(`(Intercept)` = 1, as.matrix(X_s))
  miss <- setdiff(names(cf), colnames(mm))
  if (length(miss)) {
    add <- matrix(0, nrow = nrow(mm), ncol = length(miss),
                  dimnames = list(NULL, miss))
    mm <- cbind(mm, add)
  }
  mm <- mm[, names(cf), drop = FALSE]
  as.numeric(mm %*% cf)
}

# build X/y; drop date, current_fx_level, fx_level_fwd_*, and y itself
make_xy <- function(df, y_col, date_col = "date") {
  stopifnot(y_col %in% names(df), date_col %in% names(df))
  drop_cols <- c(date_col, "current_fx_level",
                 grep("^fx_level_fwd", names(df), value = TRUE),
                 y_col)
  X <- df %>% select(-all_of(drop_cols)) %>% as.matrix()
  y <- df[[y_col]]
  dates <- as.Date(df[[date_col]])
  list(X = X, y = y, dates = dates)
}

# split, zero-variance prune, QR prune, and scale by TRAIN only
prep_split <- function(X, y, dates, split_date) {
  split_date <- as.Date(split_date)
  is_test <- dates >= split_date
  is_train <- !is_test

  X_tr <- X[is_train, , drop = FALSE]; y_tr <- y[is_train]
  X_te <- X[is_test,  , drop = FALSE]; y_te <- y[is_test]

  # zero variance on TRAIN
  sds <- apply(X_tr, 2, sd)
  keep0 <- which(is.finite(sds) & sds > 0)
  X_tr <- X_tr[, keep0, drop = FALSE]
  X_te <- X_te[, keep0, drop = FALSE]

  # QR prune linear dependencies on TRAIN 
  qr_keep <- qr_prune_cols(X_tr, X_te)
  X_tr <- qr_keep$X_tr; X_te <- qr_keep$X_te

  cn <- make.names(colnames(X_tr), unique = TRUE)
  colnames(X_tr) <- cn; colnames(X_te) <- cn

  # standardize 
  mu  <- colMeans(X_tr)
  sdv <- apply(X_tr, 2, sd)
  X_tr_s <- scale_with(X_tr, mu, sdv)
  X_te_s <- scale_with(X_te, mu, sdv)

  list(X_tr_s = X_tr_s, y_tr = y_tr, X_te_s = X_te_s, y_te = y_te, cols = cn,
       n_train = nrow(X_tr_s), n_test = nrow(X_te_s))
}
```

### Models

```{r}
# 1) OLS
fit_predict_ols <- function(X_tr_s, y_tr, X_te_s) {
  cat("  [OLS] fitting...\n")
  df_tr <- data.frame(y = y_tr, as.data.frame(X_tr_s))
  df_te <- data.frame(as.data.frame(X_te_s))
  fit   <- lm(y ~ ., data = df_tr)  # columns already QR-pruned
  list(
    pred_tr = as.numeric(predict(fit, newdata = df_tr)),
    pred_te = as.numeric(predict(fit, newdata = df_te)),
    selected = colnames(X_tr_s)
  )
}

# 2) Forward stepwise + AIC/BIC 
fit_predict_subset_forward_ic <- function(X_tr_s, y_tr, X_te_s,
                                          criterion = c("BIC","AIC"),
                                          nvmax = 25) {
  crit <- match.arg(criterion)
  stopifnot(identical(colnames(X_tr_s), colnames(X_te_s)))
  n  <- nrow(X_tr_s); p <- ncol(X_tr_s); nv <- min(nvmax, p)

  cat(sprintf("  [Subset-%s] forward selection (nvmax=%d)...\n", crit, nv))

  df_tr  <- data.frame(y = y_tr, as.data.frame(X_tr_s))
  regfit <- regsubsets(y ~ ., data = df_tr, nvmax = nv, method = "forward")
  s      <- summary(regfit)

  # choose k by BIC or AIC 
  k <- if (crit == "BIC") {
    which.min(s$bic)
  } else {
    rss <- s$rss; ks <- 1:nv
    which.min(n * log(rss / n) + 2 * (ks + 1))
  }

  pred_tr <- safe_coef_predict(regfit, X_tr_s, k)
  pred_te <- safe_coef_predict(regfit, X_te_s, k)
  vars    <- setdiff(names(coef(regfit, id = k)), "(Intercept)")
  list(pred_tr = pred_tr, pred_te = pred_te, selected = vars, k = k)
}

# 3) LASSO(BIC on glmnet path)  &  4) Post-LASSO(BIC)
fit_predict_lasso_bic <- function(X_tr_s, y_tr, X_te_s, alpha = 1) {
  stopifnot(identical(colnames(X_tr_s), colnames(X_te_s)))
  cat("  [LASSO(BIC)] glmnet path & BIC selection...\n")

  path <- glmnet(X_tr_s, y_tr, alpha = alpha, standardize = FALSE, intercept = TRUE)
  lam  <- path$lambda
  B    <- as.matrix(coef(path, s = lam))     # (p+1) x L
  # fitted on TRAIN for every lambda
  Fhat <- cbind(1, as.matrix(X_tr_s)) %*% B
  resid <- sweep(matrix(rep(y_tr, length(lam)), ncol = length(lam)), 2, Fhat, FUN = "-")
  rss   <- colSums(resid^2)
  n     <- nrow(X_tr_s)
  k_nonzero <- colSums(abs(B[-1, , drop = FALSE]) > 0)
  bic   <- n * log(rss / n) + (k_nonzero + 1) * log(n)   # +1 intercept
  jstar <- which.min(bic); lam_star <- lam[jstar]

  # penalized predictions (pure LASSO)
  pred_tr <- as.numeric(Fhat[, jstar])
  pred_te <- as.numeric(predict(path, newx = X_te_s, s = lam_star, type = "response"))
  sel     <- colnames(X_tr_s)[which(B[-1, jstar] != 0)]

  list(pred_tr = pred_tr, pred_te = pred_te, selected = sel,
       lambda_star = lam_star, beta = as.numeric(B[-1, jstar]), intercept = as.numeric(B[1, jstar]))
}

fit_predict_post_lasso_bic <- function(X_tr_s, y_tr, X_te_s, alpha = 1) {
  cat("  [Post-LASSO(BIC)] select by lasso(BIC) then refit OLS...\n")
  lasso <- fit_predict_lasso_bic(X_tr_s, y_tr, X_te_s, alpha = alpha)
  sel_idx <- which(lasso$beta != 0)
  if (!length(sel_idx)) {
    mu <- mean(y_tr)
    return(list(pred_tr = rep(mu, nrow(X_tr_s)),
                pred_te = rep(mu, nrow(X_te_s)),
                selected = character(0),
                lambda_star = lasso$lambda_star))
  }
  Xtr <- X_tr_s[, sel_idx, drop = FALSE]
  Xte <- X_te_s[, sel_idx, drop = FALSE]
  dftr <- data.frame(y = y_tr, as.data.frame(Xtr))
  fit  <- lm(y ~ ., data = dftr)
  list(
    pred_tr = as.numeric(predict(fit, newdata = dftr)),
    pred_te = as.numeric(predict(fit, newdata = data.frame(as.data.frame(Xte)))),
    selected = colnames(Xtr),
    lambda_star = lasso$lambda_star
  )
}

# 5) Ridge(BIC) via glmnet path with BIC using effective df
fit_predict_ridge_bic <- function(X_tr_s, y_tr, X_te_s) {
  stopifnot(identical(colnames(X_tr_s), colnames(X_te_s)))
  cat("  [Ridge(BIC)] glmnet path & BIC selection (effective df)...\n")

  path <- glmnet(X_tr_s, y_tr, alpha = 0, standardize = FALSE, intercept = TRUE)
  lam  <- path$lambda
  B    <- as.matrix(coef(path, s = lam))
  Fhat <- cbind(1, as.matrix(X_tr_s)) %*% B
  resid <- sweep(matrix(rep(y_tr, length(lam)), ncol = length(lam)), 2, Fhat, FUN = "-")
  rss   <- colSums(resid^2)
  n     <- nrow(X_tr_s)

  # effective df for ridge: sum s_i^2/(s_i^2 + lambda_adj)
  # NOTE: glmnet's lambda scale differs by 1/n in the loss; this df is an approximation,
  # but works well for ranking lambdas in practice.
  svals <- svd(X_tr_s, nu = 0, nv = 0)$d
  df_eff <- vapply(lam, function(l) sum(svals^2 / (svals^2 + l)), numeric(1))
  bic <- n * log(rss / n) + (1 + df_eff) * log(n)   # +1 intercept
  jstar <- which.min(bic); lam_star <- lam[jstar]

  pred_tr <- as.numeric(Fhat[, jstar])
  pred_te <- as.numeric(predict(path, newx = X_te_s, s = lam_star, type = "response"))
  list(pred_tr = pred_tr, pred_te = pred_te, selected = colnames(X_tr_s), lambda_star = lam_star)
}

# 6) Elastic Net (BIC) over a grid of alpha values
fit_predict_en_bic <- function(
  X_tr_s, y_tr, X_te_s,
  alpha_grid = c(0.1, 0.5, 0.9)
) {
  stopifnot(identical(colnames(X_tr_s), colnames(X_te_s)))
  best <- list(bic = Inf, alpha = NA_real_, j = NA_integer_,
               path = NULL, B = NULL, Fhat = NULL)

  for (a in alpha_grid) {
    path <- glmnet::glmnet(X_tr_s, y_tr, alpha = a, standardize = FALSE, intercept = TRUE)
    lam  <- path$lambda

    # coefficients across the full path: (p+1) x L
    B    <- as.matrix(coef(path, s = lam))
    Fhat <- cbind(1, as.matrix(X_tr_s)) %*% B
    Ymat  <- matrix(rep(y_tr, length(lam)), ncol = length(lam))
    resid <- Ymat - Fhat
    rss   <- colSums(resid^2)


    n   <- nrow(X_tr_s)
    dfk <- colSums(abs(B[-1, , drop = FALSE]) > 0) + 1 # +1 for intercept
    bic <- n * log(rss / n) + dfk * log(n)

    jstar <- which.min(bic)
    if (bic[jstar] < best$bic) {
      best <- list(bic = bic[jstar], alpha = a, j = jstar,
                   path = path, B = B, Fhat = Fhat)
    }
  }

  # Extract best lam and predict
  lam_star <- best$path$lambda[best$j]
  pred_tr  <- as.numeric(best$Fhat[, best$j])
  pred_te  <- as.numeric(predict(best$path, newx = X_te_s, s = lam_star, type = "response"))
  sel_idx  <- which(best$B[-1, best$j] != 0)
  sel_vars <- colnames(X_tr_s)[sel_idx]

  list(
    pred_tr      = pred_tr,
    pred_te      = pred_te,
    selected     = sel_vars,
    alpha_star   = best$alpha,
    lambda_star  = lam_star,
    beta         = as.numeric(best$B[-1, best$j]),
    intercept    = as.numeric(best$B[1,  best$j])
  )
}

```

### Evaluation
```{r}
summarize_block <- function(name, pr, y_tr, y_te) {
  data.frame(
    model   = name,
    rmse_tr = rmse(pr$pred_tr, y_tr),
    rmse_te = rmse(pr$pred_te, y_te),
    r2_in   = r2_insample(y_tr, pr$pred_tr),
    r2_oos  = r2_oos_trainmean(y_te, pr$pred_te, y_tr),
    stringsAsFactors = FALSE
  )
}

dm_p <- function(e1, e2, h = 1, power = 2) {
  tryCatch(forecast::dm.test(e1, e2, h = h, power = power)$p.value,
           error = function(e) NA_real_)
}
dm_stat <- function(e1, e2, h = 1, power = 2) {
  tryCatch(unname(forecast::dm.test(e1, e2, h = h, power = power)$statistic),
           error = function(e) NA_real_)
}

evaluate_static <- function(df, y_col, split_date = "2013-01-01", nvmax = 25, en_alpha_grid = c(0.5)) {
  cat(sprintf("\n========== Static split @ %s for %s ==========\n", split_date, y_col))
  xy <- make_xy(df, y_col); sp <- prep_split(xy$X, xy$y, xy$dates, split_date)

  cat("Preprocessed dims: ",
      sprintf("train=%d x %d, test=%d x %d\n",
              nrow(sp$X_tr_s), ncol(sp$X_tr_s), nrow(sp$X_te_s), ncol(sp$X_te_s)))

  # fits
  ols      <- fit_predict_ols(sp$X_tr_s, sp$y_tr, sp$X_te_s)
  sub_bic  <- fit_predict_subset_forward_ic(sp$X_tr_s, sp$y_tr, sp$X_te_s, "BIC", nvmax)
  sub_aic  <- fit_predict_subset_forward_ic(sp$X_tr_s, sp$y_tr, sp$X_te_s, "AIC", nvmax)
  lasso    <- fit_predict_lasso_bic(sp$X_tr_s, sp$y_tr, sp$X_te_s, alpha = 1)
  postlas  <- fit_predict_post_lasso_bic(sp$X_tr_s, sp$y_tr, sp$X_te_s, alpha = 1)
  ridge    <- fit_predict_ridge_bic(sp$X_tr_s, sp$y_tr, sp$X_te_s)
  enet    <- fit_predict_en_bic(sp$X_tr_s, sp$y_tr, sp$X_te_s, alpha_grid = en_alpha_grid)


  cat("Computing summary metrics (RMSE/R^2/DA/DM)...\n")
  tbl <- bind_rows(
    summarize_block("OLS",          ols,     sp$y_tr, sp$y_te),
    summarize_block("Subset-BIC",   sub_bic, sp$y_tr, sp$y_te),
    summarize_block("Subset-AIC",   sub_aic, sp$y_tr, sp$y_te),
    summarize_block("LASSO(BIC)",   lasso,   sp$y_tr, sp$y_te),
    summarize_block("Post-LASSO",   postlas, sp$y_tr, sp$y_te),
    summarize_block("Ridge(BIC)",   ridge,   sp$y_tr, sp$y_te),
    summarize_block("ElasticNet(BIC)", enet,   sp$y_tr, sp$y_te)

  )

  preds_list <- list(
    "OLS" = ols$pred_te, "Subset-BIC" = sub_bic$pred_te, "Subset-AIC" = sub_aic$pred_te,
    "LASSO(BIC)" = lasso$pred_te, "Post-LASSO" = postlas$pred_te, "Ridge(BIC)" = ridge$pred_te,      "ElasticNet(BIC)" = enet$pred_te

  )
  da_rows <- lapply(names(preds_list), function(m) {
    da <- dir_acc(sp$y_te, preds_list[[m]], count_zeros = FALSE)
    data.frame(model = m, da_test = da$da, da_n = da$n, da_pval = da$pval)
  })
  tbl <- left_join(tbl, bind_rows(da_rows), by = "model")

  # DM against RW(0)
  rw <- rep(0, length(sp$y_te))
  dm_res <- list(
    OLS        = list(statistic = dm_stat(sp$y_te - ols$pred_te,     sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - ols$pred_te,     sp$y_te - rw)),
    Subset_BIC = list(statistic = dm_stat(sp$y_te - sub_bic$pred_te, sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - sub_bic$pred_te, sp$y_te - rw)),
    Subset_AIC = list(statistic = dm_stat(sp$y_te - sub_aic$pred_te, sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - sub_aic$pred_te, sp$y_te - rw)),
    LASSO_BIC  = list(statistic = dm_stat(sp$y_te - lasso$pred_te,   sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - lasso$pred_te,   sp$y_te - rw)),
    PostLASSO  = list(statistic = dm_stat(sp$y_te - postlas$pred_te, sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - postlas$pred_te, sp$y_te - rw)),
    Ridge_BIC  = list(statistic = dm_stat(sp$y_te - ridge$pred_te,   sp$y_te - rw),
                      p.value   = dm_p   (sp$y_te - ridge$pred_te,   sp$y_te - rw))
  )


  list(
    summary  = tbl,
        selected = list(
      subset_bic = sub_bic$selected, subset_aic = sub_aic$selected,
      lasso      = lasso$selected,   post_lasso = postlas$selected,
      enet       = enet$selected
    ),
    enet_meta = list(alpha_star = enet$alpha_star, lambda_star = enet$lambda_star),
    preds_te = preds_list,
    y_te     = sp$y_te,
    dm       = dm_res,
    meta     = list(cols = sp$cols, n_train = sp$n_train, n_test = sp$n_test)
  )
}

# rolling window: constant width = initial train size (drop oldest, add newest each step)
evaluate_rolling <- function(df, y_col, split_date = "2013-01-01",
                             nvmax = 25, verbose_every = 10, en_alpha_grid = c(0.5)) {
  cat(sprintf("\n========== Rolling window @ %s for %s ==========\n", split_date, y_col))
  xy <- make_xy(df, y_col)
  dates <- xy$dates
  split_date <- as.Date(split_date)
  ord <- order(dates) # ensure chronological
  X <- xy$X[ord, , drop = FALSE]; y <- xy$y[ord]; dates <- dates[ord]

  n_train0 <- sum(dates < split_date)
  n_total  <- length(y)
  n_test   <- n_total - n_train0
  stopifnot(n_train0 > 0, n_test > 0)

  cat(sprintf("Rolling window: train_width=%d, test=%d steps\n", n_train0, n_test))

  # pre-alloc predictions
  models <- c("OLS","Subset-BIC","Subset-AIC","LASSO(BIC)","Post-LASSO","Ridge(BIC)","ElasticNet(BIC)")
  pred_mat <- matrix(NA_real_, nrow = n_test, ncol = length(models))
  colnames(pred_mat) <- models
  y_te <- rep(NA_real_, n_test)

  for (t in seq_len(n_test)) {
    if (t %% verbose_every == 1 || t == n_test) cat(sprintf("  [Rolling] step %d / %d\n", t, n_test))

    # training window indices (fixed width), then predict next point
    idx_tr <- t:(t + n_train0 - 1)
    idx_te <- t + n_train0
    y_te[t] <- y[idx_te]

    X_tr <- X[idx_tr, , drop = FALSE]; y_tr <- y[idx_tr]
    X_ts <- X[idx_te, , drop = FALSE]          # <-- fixed trailing comma bug

    # zero-variance prune
    sds <- apply(X_tr, 2, sd)
    keep0 <- which(is.finite(sds) & sds > 0)
    X_tr <- X_tr[, keep0, drop = FALSE]; X_ts <- X_ts[, keep0, drop = FALSE]

    # QR prune exact multicollinearity
    pr <- qr_prune_cols(X_tr, X_ts); X_tr <- pr$X_tr; X_ts <- pr$X_te

    # names & scaling (no leakage)
    cn <- make.names(colnames(X_tr), unique = TRUE)
    colnames(X_tr) <- cn; colnames(X_ts) <- cn
    mu <- colMeans(X_tr); sdv <- apply(X_tr, 2, sd)
    X_tr_s <- scale_with(X_tr, mu, sdv)
    X_ts_s <- scale_with(X_ts, mu, sdv)

    nwin <- length(y_tr)

    # ---------- OLS ----------
    pred_mat[t, "OLS"] <- {
      dftr <- data.frame(y = y_tr, as.data.frame(X_tr_s))
      predict(lm(y ~ ., data = dftr), newdata = data.frame(as.data.frame(X_ts_s)))
    }

    # ---------- Subset (BIC / AIC) ----------
    dftr <- data.frame(y = y_tr, as.data.frame(X_tr_s))
    nv   <- max(1L, min(ncol(X_tr_s), nvmax, nrow(X_tr_s) - 1L))  # keep DoF > 0
    regfit <- regsubsets(y ~ ., data = dftr, nvmax = nv, method = "forward")
    s <- summary(regfit)

    # BIC
    k_bic <- which.min(s$bic)
    pred_mat[t, "Subset-BIC"] <- safe_coef_predict(regfit, X_ts_s, k_bic)

    # AIC: AIC_k = n*log(RSS_k/n) + 2*(k+1)
    rss <- s$rss; ks <- 1:nv
    aic_vals <- nwin * log(rss / nwin) + 2 * (ks + 1)
    k_aic <- which.min(aic_vals)
    pred_mat[t, "Subset-AIC"] <- safe_coef_predict(regfit, X_ts_s, k_aic)

    # ---------- LASSO(BIC) ----------
    lfit <- glmnet::glmnet(X_tr_s, y_tr, alpha = 1, standardize = FALSE, intercept = TRUE)
    # coefficients for the whole lambda path:
    B <- as.matrix(predict(lfit, type = "coefficients", s = lfit$lambda))  # (p+1) x L
    Ftr <- cbind(1, as.matrix(X_tr_s)) %*% B
    rssL <- colSums((y_tr - Ftr)^2)
    k_nz <- colSums(abs(B[-1, , drop = FALSE]) > 0)
    bicL <- nwin * log(rssL / nwin) + (k_nz + 1) * log(nwin)
    j_star <- which.min(bicL)                              # <-- fixed name (no j*)
    b_star <- B[, j_star, drop = FALSE]
    pred_mat[t, "LASSO(BIC)"] <- as.numeric(c(1, as.numeric(X_ts_s)) %*% b_star)

    # ---------- Post-LASSO (OLS on selected vars) ----------
    sel <- which(abs(b_star[-1, 1]) > 0)
    if (length(sel) > 0 && length(sel) < nrow(X_tr_s)) {
      dftr_sub <- data.frame(y = y_tr, as.data.frame(X_tr_s[, sel, drop = FALSE]))
      dfte_sub <- data.frame(as.data.frame(X_ts_s[, sel, drop = FALSE]))
      fit_post <- lm(y ~ ., data = dftr_sub)
      pred_mat[t, "Post-LASSO"] <- as.numeric(predict(fit_post, newdata = dfte_sub))
    } else {
      pred_mat[t, "Post-LASSO"] <- mean(y_tr)  # fallback
    }

    # ---------- Ridge(BIC) ----------
    rfit <- glmnet::glmnet(X_tr_s, y_tr, alpha = 0, standardize = FALSE, intercept = TRUE)
    BR   <- as.matrix(predict(rfit, type = "coefficients", s = rfit$lambda))
    FtrR <- cbind(1, as.matrix(X_tr_s)) %*% BR
    rssR <- colSums((y_tr - FtrR)^2)
    # effective df per lambda from glmnet (excludes intercept -> add 1 in BIC)
    k_eff <- rfit$df
    bicR  <- nwin * log(rssR / nwin) + (k_eff + 1) * log(nwin)
    jr <- which.min(bicR)
    br_star <- BR[, jr, drop = FALSE]
    pred_mat[t, "Ridge(BIC)"] <- as.numeric(c(1, as.numeric(X_ts_s)) %*% br_star)
  }
  
    # ---------- Elastic Net (BIC over alphas) ----------
    best_en <- list(bic = Inf, pred = NA_real_)
    for (a in en_alpha_grid) {
      en_path <- glmnet::glmnet(X_tr_s, y_tr, alpha = a, standardize = FALSE, intercept = TRUE)
      lam     <- en_path$lambda
      B_en    <- as.matrix(predict(en_path, type = "coefficients", s = lam))  # (p+1) x L
      Ftr_en  <- cbind(1, as.matrix(X_tr_s)) %*% B_en

      # safe residuals & BIC
      Ymat_en  <- matrix(rep(y_tr, length(lam)), ncol = length(lam))
      rss_en   <- colSums((Ymat_en - Ftr_en)^2)
      df_en    <- colSums(abs(B_en[-1, , drop = FALSE]) > 0) + 1
      bic_en   <- nwin * log(rss_en / nwin) + df_en * log(nwin)

      j_en <- which.min(bic_en)
      b_en <- B_en[, j_en, drop = FALSE]
      pred_en <- as.numeric(c(1, as.numeric(X_ts_s)) %*% b_en)

      if (bic_en[j_en] < best_en$bic) {
        best_en <- list(bic = bic_en[j_en], pred = pred_en)
      }
    }
    pred_mat[t, "ElasticNet(BIC)"] <- best_en$pred


  # metrics for rolling predictions
  cat("Computing rolling metrics (RMSE/DA/DM)...\n")
  y_pred   <- as.data.frame(pred_mat)
  rmse_vec <- sapply(y_pred, function(p) rmse(p, y_te))

  da_tbl <- lapply(names(y_pred), function(m) {
    da <- dir_acc(y_te, y_pred[[m]], count_zeros = FALSE)
    data.frame(model = m, rmse_roll = rmse_vec[[m]],
               da_roll = da$da, da_n = da$n, da_pval = da$pval)
  }) %>% bind_rows()

  rw <- rep(0, length(y_te))
  dm_res <- lapply(names(y_pred), function(m) {
    e1 <- y_te - y_pred[[m]]
    e2 <- y_te - rw
    list(statistic = dm_stat(e1, e2, h = 1, power = 2),
         p.value   = dm_p  (e1, e2, h = 1, power = 2))
  })
names(dm_res) <- names(y_pred)

  names(dm_res) <- names(y_pred)

  list(
    preds   = y_pred,
    y_te    = y_te,
    summary = da_tbl,
    dm      = dm_res,
    meta    = list(n_train0 = n_train0, n_test = n_test)
  )
}

```

```{r}
run_all_for_df <- function(df, y_col, split_date = "2013-01-01", nvmax = 25) {
  list(
    static  = evaluate_static(df, y_col, split_date = split_date, nvmax = nvmax),
    rolling = evaluate_rolling(df, y_col, split_date = split_date, nvmax = nvmax)
  )
}
```

```{r}
split_date <- "2013-01-01"

res_1m <- run_all_for_df(df_1m,  "Y_log_return_1m",  split_date, nvmax = 25)


# peek
res_1m$static$summary
res_1m$rolling$summary

```

```{r}
run_train_test <- function(df, y_col, split_date = "2013-01-01", nvmax = 25) {
  evaluate_static(df, y_col, split_date = split_date, nvmax = nvmax)
}

# Example:
res_1m_tt  <- run_train_test(df_1m,  "Y_log_return_1m",  "2013-01-01", nvmax = 25)
res_3m_tt  <- run_train_test(df_3m,  "Y_log_return_3m",  "2013-01-01", nvmax = 25)
res_6m_tt  <- run_train_test(df_6m,  "Y_log_return_6m",  "2013-01-01", nvmax = 25)
res_12m_tt <- run_train_test(df_12m, "Y_log_return_12m", "2013-01-01", nvmax = 25)
```

```{r}
# Inspect
res_12m_tt$summary
res_12m_tt$selected
res_12m_tt$dm$OLS
```

```{r}
# Static only, exploring alphas:
res_1m_tt <- evaluate_static(df_1m, "Y_log_return_1m", split_date = "2013-01-01",
                             nvmax = 25, en_alpha_grid = c(0.1, 0.3, 0.5, 0.7, 0.9))

# Rolling with the same alpha grid:
res_1m_roll <- evaluate_rolling(df_1m, "Y_log_return_1m", split_date = "2013-01-01",
                                nvmax = 25, verbose_every = 10,
                                en_alpha_grid = c(0.1, 0.3, 0.5, 0.7, 0.9))
```

```{r}
res_3m  <- run_all_for_df(df_3m,  y_col = "Y_log_return_3m",  split_date = split_date, nvmax = 25)
res_6m  <- run_all_for_df(df_6m,  y_col = "Y_log_return_6m",  split_date = split_date, nvmax = 25)
res_12m <- run_all_for_df(df_12m, y_col = "Y_log_return_12m", split_date = split_date, nvmax = 25)
```
```{r}
res_12m$rolling$summary

```

